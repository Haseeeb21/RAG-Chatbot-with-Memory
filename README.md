# RAG-Chatbot-with-Memory

A production-ready Retrieval-Augmented Generation (RAG) chatbot with per-user conversation memory. Query your documents (PDF, TXT, DOCX) using natural language with ChromaDB vector search and OpenAI embeddings.

## âœ¨ Features

- ğŸ“š **Document Processing**: Support for PDF, TXT, and DOCX files
- ğŸ” **Semantic Search**: ChromaDB vector database with cosine similarity
- ğŸ§  **User Memory**: Per-user conversation history with sliding window
- ğŸš€ **Fast API**: RESTful API with FastAPI and automatic documentation
- ğŸ” **Local Storage**: Documents and conversations stored locally
- ğŸ’¬ **Context-Aware**: Maintains conversation flow and understands follow-up questions
- ğŸ“Š **Source Attribution**: Cites source documents in responses

## ğŸ—ï¸ Architecture

```
User Query â†’ Embedding â†’ Vector Search â†’ Context Building â†’ LLM â†’ Response
                â†“                            â†“
          ChromaDB (Local)          Conversation Memory (Local)
```

**Tech Stack:**
- **Backend**: FastAPI
- **Vector DB**: ChromaDB (local, embedded)
- **LLM**: OpenAI GPT-4, etc.
- **Embeddings**: OpenAI text-embedding-3-small
- **Document Processing**: LangChain, PyPDF, python-docx

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10 or higher
- OpenAI API key 

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/haseeeb21/rag-chatbot-with-memory.git
cd rag-chatbot-with-memory
```

2. **Create virtual environment**
```bash
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# Mac/Linux:
source venv/bin/activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure environment**
```bash
# Copy example env file
cp .env.example .env

# Edit .env and add your OpenAI API key
OPENAI_API_KEY=sk-your-api-key-here
```

5. **Add your documents**
```bash
# Place your PDF, TXT, DOCX files in:
data/documents/
```

6. **Run the server**
```bash
python -m app.main
```

Server will start at: `http://localhost:8000`

Interactive docs: `http://localhost:8000/docs`

## ğŸ“– Usage

### 1. Index Your Documents

First, index your documents to create the knowledge base:

```bash
curl -X POST http://localhost:8000/index
```

**Response:**
```json
{
  "message": "Documents indexed successfully",
  "processed_files": ["document1.pdf", "document2.txt"],
  "total_chunks": 45,
  "status": "completed"
}
```

### 2. Query the System

Ask questions about your documents:

```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "ali",
    "query": "What are the main topics in the documents?"
  }'
```

**Response:**
```json
{
  "answer": "Based on the documents, the main topics include...",
  "retrieved_documents": [
    {
      "content": "Relevant excerpt...",
      "metadata": {"filename": "document1.pdf"},
      "relevance_score": 0.89
    }
  ],
  "conversation_id": "ali",
  "timestamp": "2024-12-31T10:30:00"
}
```

### 3. Follow-up Questions

The system remembers your conversation:

```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "ali",
    "query": "Can you explain more about that?"
  }'
```

## ğŸ”Œ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | Health check |
| `POST` | `/query` | Ask a question |
| `POST` | `/index` | Index documents |
| `GET` | `/history/{user_id}` | Get conversation history |
| `DELETE` | `/history/{user_id}` | Clear user history |
| `GET` | `/stats` | System statistics |
| `DELETE` | `/clear-db` | Clear vector database |

### Full API Documentation

Visit `http://localhost:8000/docs` for interactive Swagger UI documentation.

## ğŸ§ª Testing with Postman

1. **Import Collection**: Use the provided `/openapi.json` in the docs page.
2. **Set Base URL**: `http://localhost:8000`
3. **Test Sequence**:
   - POST `/index` - Index documents
   - POST `/query` - Ask first question
   - POST `/query` - Ask follow-up
   - GET `/history/{user_id}` - Check memory

## ğŸ“ Project Structure

```
rag-chatbot-with-memory/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ services/          # Core business logic
â”‚   â”‚   â”œâ”€â”€ document_processor.py
â”‚   â”‚   â”œâ”€â”€ embedding_service.py
â”‚   â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”‚   â”œâ”€â”€ rag_service.py
â”‚   â”‚   â””â”€â”€ memory_service.py
â”‚   â”œâ”€â”€ main.py           # FastAPI application
â”‚   â”œâ”€â”€ models.py         # Pydantic models
â”‚   â””â”€â”€ config.py         # Configuration
â”œâ”€â”€ data/
â”‚   â””â”€â”€ documents/        # Your documents (PDF, TXT, DOCX)
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ chroma_db/        # Vector database
â”‚   â””â”€â”€ conversations/    # User conversation history
â””â”€â”€ requirements.txt
```

## ğŸ”§ Configuration

All settings can be configured in `.env`:

| Variable | Default | Description |
|----------|---------|-------------|
| `OPENAI_API_KEY` | - | Your OpenAI API key |
| `EMBEDDING_MODEL` | text-embedding-3-small | Embedding model |
| `LLM_MODEL` | gpt-4-turbo-preview | Language model |
| `CHUNK_SIZE` | 1000 | Document chunk size |
| `CHUNK_OVERLAP` | 200 | Overlap between chunks |
| `MAX_CONVERSATION_HISTORY` | 10 | Messages to keep in memory |

## ğŸ“Š How It Works

### 1. Document Indexing Pipeline
```
Documents â†’ Text Extraction â†’ Chunking â†’ Embeddings â†’ Vector Store
```

### 2. Query Processing Pipeline
```
Query â†’ Embedding â†’ Similarity Search â†’ Context Building â†’ LLM â†’ Response
          â†“                                   â†“
    ChromaDB Search                  Conversation History
```

### 3. Memory Management
- Per-user conversation storage
- Sliding window (keeps last N messages)
- Persistent to disk
- Included in context for follow-ups

## ğŸ¯ Use Cases

- **Personal Knowledge Base**: Query your research papers, notes, and documents
- **Document Q&A**: Get answers from technical documentation
- **Customer Support**: Build a chatbot for your product docs
- **Research Assistant**: Analyze multiple documents and extract insights
- **Legal/Medical Document Analysis**: Query domain-specific documents

## ğŸ”’ Privacy & Security

- âœ… Documents stored locally
- âœ… Conversation history stored locally
- âœ… Only embeddings sent to OpenAI (not full documents)
- âš ï¸ API calls to OpenAI (queries and responses)
- ğŸ” No data shared between users

## ğŸš§ Limitations

- Requires OpenAI API (not fully offline)
- Limited by OpenAI rate limits
- Vector database size depends on document count
- Context window limited by LLM


## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- [FastAPI](https://fastapi.tiangolo.com/) - Modern web framework
- [ChromaDB](https://www.trychroma.com/) - Vector database
- [LangChain](https://www.langchain.com/) - LLM framework
- [OpenAI](https://openai.com/) - LLM and embeddings


---

â­ If you find this project useful, please consider giving it a star on GitHub!
